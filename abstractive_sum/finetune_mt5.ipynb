{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "finetune_mt5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b5eedbf3eabc4001a5356556ec062b36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d05d6ff368ed4fb181eb22c7f3bf4d35",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2106f784658d41aaa864af41dd02ddec",
              "IPY_MODEL_631aca92bc4d42a9ab368f017436a24d"
            ]
          }
        },
        "d05d6ff368ed4fb181eb22c7f3bf4d35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "2106f784658d41aaa864af41dd02ddec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e46474af7e554e778f0817d8f4a82d10",
            "_dom_classes": [],
            "description": "Validation sanity check:   0%",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 2,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c8e4ca5edba845e59ba06e622a947bf0"
          }
        },
        "631aca92bc4d42a9ab368f017436a24d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_806f658171fa4096a8075173b8c99c49",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/2 [00:00&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_92f9ed8654c342fd88eeee9b63e94a9e"
          }
        },
        "e46474af7e554e778f0817d8f4a82d10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c8e4ca5edba845e59ba06e622a947bf0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "806f658171fa4096a8075173b8c99c49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "92f9ed8654c342fd88eeee9b63e94a9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "50023167cd454605a202f967ca5b7320": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_90f2ede666c841ffa8ac97958dbe9197",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8c8e750dbe1b4678825532c9b8f3c49c",
              "IPY_MODEL_086fd29f17744fe897d2313c4268c75f"
            ]
          }
        },
        "90f2ede666c841ffa8ac97958dbe9197": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "8c8e750dbe1b4678825532c9b8f3c49c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b51d3f5855b4424b9d5eb46a90e0b5f1",
            "_dom_classes": [],
            "description": "Epoch 0:   0%",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 57665,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 120,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_296c09f5c05a44c293aff423882284c5"
          }
        },
        "086fd29f17744fe897d2313c4268c75f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2d176b3152d74f5ba603e7b827ae7b87",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 120/57665 [00:31&lt;4:11:08,  3.82it/s, loss=11.3, v_num=1]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_edf948b8699b4a1baf90825940898fcb"
          }
        },
        "b51d3f5855b4424b9d5eb46a90e0b5f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "296c09f5c05a44c293aff423882284c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2d176b3152d74f5ba603e7b827ae7b87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "edf948b8699b4a1baf90825940898fcb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFstgefnb9F6"
      },
      "source": [
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip install pytorch_lightning==1.2.2\n",
        "!pip install sentencepiece==0.1.85\n",
        "!pip install torch==1.7.1\n",
        "!pip install torchvision==0.8.2\n",
        "!pip install torchtext==0.8.0\n",
        "!pip install fairseq\n",
        "!pip install tensorflow_datasets -U\n",
        "!pip install datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fBAZ9Skb9F6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d1fcf47-36b5-4e6b-fadd-a42474c1f071"
      },
      "source": [
        "import argparse\n",
        "import glob\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import logging\n",
        "import random\n",
        "import re\n",
        "from itertools import chain\n",
        "from string import punctuation\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pytorch_lightning as pl"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kM83UJ_-X33"
      },
      "source": [
        "!tar -xf gazeta_jsonl.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uxvxgavb9F8"
      },
      "source": [
        "from transformers import (\n",
        "    AdamW,\n",
        "    MT5ForConditionalGeneration, \n",
        "    AutoTokenizer,\n",
        "    get_linear_schedule_with_warmup\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xA77EXhWPCcf"
      },
      "source": [
        "# training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJDBJL6yb9F8"
      },
      "source": [
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2p-IxE3tb9F9"
      },
      "source": [
        "from fairseq.optim.adafactor import Adafactor\n",
        "class MT5FineTuner(pl.LightningModule):\n",
        "    def __init__(self, hparams):\n",
        "        super(MT5FineTuner, self).__init__()\n",
        "        self.hparams = hparams\n",
        "\n",
        "        self.model = MT5ForConditionalGeneration.from_pretrained(hparams.model_name_or_path)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(hparams.tokenizer_name_or_path)\n",
        "\n",
        "    def is_logger(self):\n",
        "        return True\n",
        "\n",
        "    def forward(\n",
        "            self, input_ids, attention_mask=None, decoder_input_ids=None, decoder_attention_mask=None, labels=None\n",
        "    ):\n",
        "        return self.model(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            decoder_input_ids=decoder_input_ids,\n",
        "            decoder_attention_mask=decoder_attention_mask,\n",
        "            labels=labels,\n",
        "        )\n",
        "\n",
        "    def _step(self, batch):\n",
        "        labels = batch[\"target_ids\"]\n",
        "        labels[labels[:, :] == self.tokenizer.pad_token_id] = -100\n",
        "\n",
        "        outputs = self(\n",
        "            input_ids=batch[\"source_ids\"],\n",
        "            attention_mask=batch[\"source_mask\"],\n",
        "            labels=labels,\n",
        "            decoder_attention_mask=batch['target_mask']\n",
        "        )\n",
        "\n",
        "        loss = outputs[0]\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss = self._step(batch)\n",
        "        self.log('loss', loss)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        loss = self._step(batch)\n",
        "        self.log('val_loss', loss)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        \"Prepare optimizer and schedule (linear warmup and decay)\"\n",
        "\n",
        "        model = self.model\n",
        "        optimizer = Adafactor(model.parameters(), lr=0.001, scale_parameter=False, relative_step=False)\n",
        "        # optimizer = AdamW(model.parameters(), lr=0.001)\n",
        "        self.opt = optimizer\n",
        "        return [optimizer]\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        train_dataset = get_dataset(tokenizer=self.tokenizer, data_type=\"train\", args=self.hparams)\n",
        "        dataloader = DataLoader(train_dataset, batch_size=self.hparams.train_batch_size, drop_last=True, shuffle=True,\n",
        "                                num_workers=4)\n",
        "        t_total = (\n",
        "                (len(dataloader.dataset) // (self.hparams.train_batch_size * max(1, self.hparams.n_gpu)))\n",
        "                // self.hparams.gradient_accumulation_steps\n",
        "                * float(self.hparams.num_train_epochs)\n",
        "        )\n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "            self.opt, num_warmup_steps=self.hparams.warmup_steps, num_training_steps=t_total\n",
        "        )\n",
        "        self.lr_scheduler = scheduler\n",
        "        return dataloader\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        # val_dataset = get_dataset(tokenizer=self.tokenizer, data_type=\"validation\", args=self.hparams)\n",
        "        val_dataset = get_dataset(tokenizer=self.tokenizer, data_type=\"val\", args=self.hparams)\n",
        "        return DataLoader(val_dataset, batch_size=self.hparams.eval_batch_size, num_workers=4)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a3Yt3Veb9F-"
      },
      "source": [
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class LoggingCallback(pl.Callback):\n",
        "        def on_validation_end(self, trainer, pl_module):\n",
        "            logger.info(\"***** Validation results *****\")\n",
        "            if pl_module.is_logger():\n",
        "                  metrics = trainer.callback_metrics\n",
        "                  # Log results\n",
        "                  for key in sorted(metrics):\n",
        "                    if key not in [\"log\", \"progress_bar\"]:\n",
        "                      logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n",
        "\n",
        "        def on_test_end(self, trainer, pl_module):\n",
        "            logger.info(\"***** Test results *****\")\n",
        "\n",
        "            if pl_module.is_logger():\n",
        "                metrics = trainer.callback_metrics\n",
        "\n",
        "                  # Log and save results to file\n",
        "                output_test_results_file = os.path.join(pl_module.hparams.output_dir, \"test_results.txt\")\n",
        "                with open(output_test_results_file, \"w\") as writer:\n",
        "                    for key in sorted(metrics):\n",
        "                          if key not in [\"log\", \"progress_bar\"]:\n",
        "                            logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n",
        "                            writer.write(\"{} = {}\\n\".format(key, str(metrics[key])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-pXrpqQb9F_"
      },
      "source": [
        "Let's define the hyperparameters and other arguments. You can overide this dict for specific task as needed. While in most of cases you'll only need to change the data_dirand output_dir.\n",
        "\n",
        "Here the batch size is 8 and gradient_accumulation_steps are 8 so the effective batch size is 64"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5882PGe2b9F_"
      },
      "source": [
        "args = dict(\n",
        "    data_dir=\"./\", # path for data files\n",
        "    output_dir=\"./\", # path to save the checkpoints\n",
        "    model_name_or_path='google/mt5-small',\n",
        "    tokenizer_name_or_path='google/mt5-large',\n",
        "    max_seq_length=512,\n",
        "    learning_rate=1e-3,\n",
        "    weight_decay=0.0,\n",
        "    warmup_steps=0,\n",
        "    train_batch_size=1,\n",
        "    eval_batch_size=1,\n",
        "    num_train_epochs=20,\n",
        "    gradient_accumulation_steps=8,\n",
        "    n_gpu=1,\n",
        "    fp_16=False, # if you want to enable 16-bit training then install apex and set this to true\n",
        "    opt_level='O1', # you can find out more on optimisation levels here https://nvidia.github.io/apex/amp.html#opt-levels-and-properties\n",
        "    # max_grad_norm=0.5, # if you enable 16-bit training then set this to a sensible value, 0.5 is a good default\n",
        "    seed=42,\n",
        ")\n",
        "args = argparse.Namespace(**args_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6i8Qb2eb9GA"
      },
      "source": [
        "try:\n",
        "  from datasets import load_dataset\n",
        "except:\n",
        "  !pip install datasets\n",
        "  from datasets import load_dataset\n",
        "\n",
        "import copy\n",
        "class ru_sum_dataset(Dataset):\n",
        "    def __init__(self, tokenizer, data_type, max_len=30): \n",
        "        # dataset = load_dataset(\n",
        "        #   'mlsum', 'ru')\n",
        "        # self.data = dataset[data_type]\n",
        "        self.max_len = max_len\n",
        "        self.tokenizer = tokenizer\n",
        "        import json\n",
        "\n",
        "        with open(f'gazeta_{data_type}.jsonl', 'r') as json_file:\n",
        "            self.data = list(json_file)\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # print(self.inputs[index])\n",
        "        # input_text, output_text= self.data['text'][index], self.data['summary'][index]\n",
        "        splitted_row = self.data[index].split('\"')\n",
        "        input_text, output_text= splitted_row[7], splitted_row[-6]\n",
        "        input_ = \"summarize: %s\" % (input_text)\n",
        "        target = \"%s </s>\" %(output_text)\n",
        "\n",
        "        # tokenize inputs\n",
        "        tokenized_inputs = self.tokenizer.batch_encode_plus(\n",
        "            [input_], max_length=self.max_len, padding=\"max_length\", \n",
        "            truncation=True, return_tensors=\"pt\"\n",
        "        )\n",
        "        # tokenize targets\n",
        "        tokenized_targets = self.tokenizer.batch_encode_plus(\n",
        "            [target], max_length=512, padding=\"max_length\",\n",
        "            truncation=True, return_tensors=\"pt\"\n",
        "        )\n",
        "        source_ids = tokenized_inputs[\"input_ids\"].squeeze()\n",
        "        target_ids = tokenized_targets[\"input_ids\"].squeeze()\n",
        "\n",
        "        src_mask = tokenized_inputs[\"attention_mask\"].squeeze()  # might need to squeeze\n",
        "        target_mask = tokenized_targets[\"attention_mask\"].squeeze()  # might need to squeeze\n",
        "\n",
        "        return {\"source_ids\": source_ids, \"source_mask\": src_mask, \"target_ids\": target_ids, \"target_mask\": target_mask}    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFQcRFqkb9GB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08adace3-0e0b-4d58-af6e-525af6c53911"
      },
      "source": [
        "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
        "    period =1, dirpath=args.output_dir, prefix=\"checkpoint\", monitor=\"val_loss\", mode=\"min\", save_top_k=1\n",
        ")\n",
        "\n",
        "train_params = dict(\n",
        "    accumulate_grad_batches=args.gradient_accumulation_steps,\n",
        "    gpus=args.n_gpu,\n",
        "    max_epochs=args.num_train_epochs,\n",
        "    precision= 16 if args.fp_16 else 32,\n",
        "    amp_level=args.opt_level,\n",
        "    # gradient_clip_val=args.max_grad_norm,\n",
        "    # amp_backend='native',\n",
        "    checkpoint_callback=checkpoint_callback,\n",
        "    callbacks=[LoggingCallback()],\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Checkpoint directory ./ exists and is not empty.\n",
            "  warnings.warn(*args, **kwargs)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28lZpsNxb9GB"
      },
      "source": [
        "def get_dataset(tokenizer, data_type, args):\n",
        "    return ru_sum_dataset(tokenizer=tokenizer, data_type=data_type, max_len=args.max_seq_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdQNrzY3b9GC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f93dbde5-0d1c-4c18-cd04-eeb0e2c6de57"
      },
      "source": [
        "print (\"Initialize model\")\n",
        "model = MT5FineTuner(args)\n",
        "\n",
        "trainer = pl.Trainer(**train_params)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initialize model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "GPU available: True, used: True\n",
            "TPU available: None, using: 0 TPU cores\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWBamfGJb9GC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697,
          "referenced_widgets": [
            "b5eedbf3eabc4001a5356556ec062b36",
            "d05d6ff368ed4fb181eb22c7f3bf4d35",
            "2106f784658d41aaa864af41dd02ddec",
            "631aca92bc4d42a9ab368f017436a24d",
            "e46474af7e554e778f0817d8f4a82d10",
            "c8e4ca5edba845e59ba06e622a947bf0",
            "806f658171fa4096a8075173b8c99c49",
            "92f9ed8654c342fd88eeee9b63e94a9e",
            "50023167cd454605a202f967ca5b7320",
            "90f2ede666c841ffa8ac97958dbe9197",
            "8c8e750dbe1b4678825532c9b8f3c49c",
            "086fd29f17744fe897d2313c4268c75f",
            "b51d3f5855b4424b9d5eb46a90e0b5f1",
            "296c09f5c05a44c293aff423882284c5",
            "2d176b3152d74f5ba603e7b827ae7b87",
            "edf948b8699b4a1baf90825940898fcb"
          ]
        },
        "outputId": "c5c598bc-89c8-489e-96b2-31b730789277"
      },
      "source": [
        "print (\"Training model\")\n",
        "trainer.fit(model)\n",
        "print (\"training finished\")\n",
        "\n",
        "print (\"Saving model\")\n",
        "model.model.save_pretrained(\"./\")\n",
        "print (\"Saved model\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  | Name  | Type                        | Params\n",
            "------------------------------------------------------\n",
            "0 | model | MT5ForConditionalGeneration | 300 M \n",
            "------------------------------------------------------\n",
            "300 M     Trainable params\n",
            "0         Non-trainable params\n",
            "300 M     Total params\n",
            "1,200.707 Total estimated model params size (MB)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b5eedbf3eabc4001a5356556ec062b36",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "50023167cd454605a202f967ca5b7320",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
            "  warnings.warn(*args, **kwargs)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "training finished\n",
            "Saving model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-6f350e371f43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Saving model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Saved model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36msave_pretrained\u001b[0;34m(self, save_directory, save_config, state_dict, save_function, push_to_hub, **kwargs)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;31m# If we save using the predefined names, we can load using `from_pretrained`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m         \u001b[0moutput_model_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWEIGHTS_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m         \u001b[0msave_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_model_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Model weights saved in {output_model_file}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    372\u001b[0m                 \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m         \u001b[0m_legacy_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBeP9YNoR0bH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a28f5730-c3be-475d-dd4f-abc2e2018762"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Apr  4 12:21:43 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.67       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   67C    P0    43W / 250W |    935MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_3AdnvqGT5L"
      },
      "source": [
        "# inference "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhMeFJLpHIIo",
        "outputId": "235ecb4e-29f8-4658-8dd6-187870327661"
      },
      "source": [
        "!unzip mt5_large_gazeta.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/MyDrive/mt5_large_gazeta.zip\n",
            "   creating: mt5_large/\n",
            "  inflating: mt5_large/config.json   \n",
            "  inflating: mt5_large/pytorch_model.bin  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPqJSuG9IIw5"
      },
      "source": [
        "article_text = '''\n",
        "Руководство Сбербанка планирует запустить продажу лекарств в отделениях кредитной организации. Об этом сообщает РИА Новости со ссылкой на первого зампредседателя правления банка Льва Хасиса.\n",
        "Мы планируем сейчас провести пилоты по открытию физических «СберЕАптек» на площадях наших отделений банков, — приводит агентство его слова.\n",
        "Хасис отметил, что если клиентам понравится такое совмещение, то проект получит дальнейшее развитие.\n",
        "Он добавил, что помещения в отделениях Сбербанка будут предоставляться аптеке на весьма льготных условиях, поэтому сможет устанавливать низкие цены на лекарства.\n",
        "Кроме того, предлагается обговорить с производителями возможность оптимизации фасовки препаратов. В случае достижения договорённости, у покупателей будет возможность приобретать точно предписанное врачом количество таблеток, а не платить за всю упаковку, пояснил Хасис.\n",
        "Помимо всего прочего, у «СберЕАптеки» есть планы по запуску собственной линии дешёвых дженериков.\n",
        "Ранее сообщалось, что компания «Российские железные дороги» (РЖД) планирует привлечь на вокзалы Москвы сетевые компании, в том числе магазины одежды, аптеки и кафе. Это будет предусмотрено единым стандартом развития вокзальных комплексов, который будет разработан в текущем году.\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvCeNDKZMPBQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb2250cd-4ad6-4421-aad0-93265a144dfc"
      },
      "source": [
        "from transformers import MT5ForConditionalGeneration, AutoTokenizer\n",
        "\n",
        "\n",
        "model_name = \"./mt5_large\"\n",
        "model = MT5ForConditionalGeneration.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained('google/mt5-large')\n",
        "\n",
        "input_ids = tokenizer.prepare_seq2seq_batch(\n",
        "    [article_text],\n",
        "    src_lang=\"en_XX\", # fairseq training artifact\n",
        "    return_tensors=\"pt\",\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    max_length=1024\n",
        ")[\"input_ids\"]\n",
        "\n",
        "output_ids = model.generate(\n",
        "    input_ids=input_ids,\n",
        "    max_length=256,\n",
        "    no_repeat_ngram_size=3,\n",
        "    num_beams=5,\n",
        "    top_k=0\n",
        ")[0]\n",
        "\n",
        "summary = tokenizer.decode(output_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
        "print(summary)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:3255: FutureWarning: `prepare_seq2seq_batch` is deprecated and will be removed in version 5 of 🤗 Transformers. Use the regular `__call__` method to prepare your inputs and the tokenizer under the `with_target_tokenizer` context manager to prepare your targets. See the documentation of your specific tokenizer for more details\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py:965: UserWarning: `max_length` is deprecated in this function, use `stopping_criteria=StoppingCriteriaList(MaxLengthCriteria(max_length=max_length))` instead.\n",
            "  UserWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Руководство Сбербанка планирует запустить продажу лекарств в отделениях банков. По словам первого зампредседателя правления банка Льва Хасиса, если клиентам понравится такое совмещение, то проект получит дальнейшее развитие. Ранее Сбербанк планировал привлечь на вокзалы Москвы сетевые компании, в том числе магазины одежды, аптеки и кафе. \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1GTGSBGIsPh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}